---
categories: it database nosql
...

## Documentation

* cassandra.apache.org outdated
* DataStax most up to date


## Architecture 

### Overview

* many nodes
* client requests always based on a key

~~~ {.asciiart}
+------+
|Client|
|------|
|Driver|
+------+
  ^
  |
  v
+---------node 1-+     +--node 2-+
|API             |     |         |
|----------------|     |         |
|cluster-aware  <------->        |
|----------------|     |         |
|cluster-unaware |     |         |
|----------------|     |         |
|disk            |     |         |
+----------------+     +---------+
~~~

* All nodes are equal, no masters.
* Peer-to-peer over gossip protocol: no full mesh, every node picks three random nodes every second and communicates with them.
* Transport mechanism: historically: [Thrift](Apache Thrift). Since 1.2 a native binary protocol which is much more flexible and efficient. Native is recommended.
* Log storage -- no in-place updates, append only, allows for very fast writes
* manages its own storage locally -- tight integration with storage (unlike HBase), allows optimisations
* new nodes can be added while online, but it should be done before the cluster is close to running out of capacity

### Cluster Structure

A node joins a cluster based on its configuration. There is a seed list of nodes from which it will bootstrap its communication. After a while it will be aware of the entire cluster. 

The nodes are organised in a logical ring that represents the hash space in [DHT](distributed hash table). Each node receives a token that corresponds to a specific hash range. The hash function is configurable:

* Murmur3Partitioner -- fastest, recommended
* RandomPartitioner
* ByteOrderPartitioner -- deprecated, not recommended

When client sends a request it is sent to a node (which one depends on policy, random by default) and that node assumes the role of a coordinator. If that node fails another node will be contacted. The coordinator computes the partition key using hash function and communicates with the node which holds the token for that partition key (and all replicas). It will participate in all the communication between the client and nodes responsible for the data. New drivers can be aware of cluster topology and can send requests directly to node responsible for the partition.

Virtual nodes: instead of being responsible for a single token, node can get a number (256 is a good default) of tokens. It makes admin tasks such as rebalancing of the cluster much easier and spreads the load more evenly. Also useful in case of heterogeneous hardware -- more powerful servers can be assigned more vnodes.

### Replication

Replication is controlled by two parameters: *replication factor* and *replication strategy*. 

Replication factor simply determines how many replicas there are. 3 is a reasonable default. It is set per keyspace and in general should not be changed in existing cluster.

Placement of replicas is determined by replication strategy:

* SimpleStrategy -- first replicat determined by partitioner, additional are next n nodes clockwise in the ring; only suitable for single datacentre, not recommended for production
* NetworkTopologyStrategy -- rack/datacentre aware placement of replicas; specifies how many replicas in each datacentre, useful if there are e.g. different performance requirements in different datacentres.

### Consistency

Updates might not propagate to all replicas immediately. What if a read occurs before an update is propagated to all replicas? There is consistency requirement setting on writes and reads (*tunable consistency*) which determines how many replicas need to respond before we are happy with the result. This can be set at anything from *any* (we don't care how many) to *all* (all replicas have to respond, i.e. immediate consistency). *Quorum* offers strong consistency, if used for both writes and reads. In general, if *read consistency level* + *write consistency level* > *replication factor* we get strong consistency.

Levels useful in practice:

* *any* -- only applies to writes; for data that we can lose
* *one* -- high availability and performance
* *quorum* -- strong consistency
* *local quorum* -- minimises latency with within-datacentre consistency
* *each quorum* -- ensures consistency in the face of total datacentre loss

Noone uses *all* in practice; if we need that level of consistency we are typically better off with a different product.

What happens when a replica is down? Coordinator stores a hint that a given write needs to be sent and sends them when replica comes back online (*hinted handoff*). What if the coordinator dies and the hints are lost? Read-repair addresses it. Note that hinted handoff only applies to situation when consistency level of write is lower than *all*. We always have the option to retry in the app if write at high consistency level fails and we need assurance about consistency.

### Gossip

Each node has a list of seed nodes it will try to contact in order to bootstrap. Every second every node picks three random nodes and sends the latest information it has about all nodes in the cluster (e.g. their key ranges, ping time etc.). Protocol described in more detail in Dynamo paper.

### Read Repair

When read is requested with consistency lower than all, the responses from fastest replicas will be returned to the client. The coordinator will also send a repair request to other replicas; they will compare their data with a checksum provided by the fastest replicas and if there is a mismatch will attempt to repair.

## Configuration

* `conf/cassandra.yaml`. Main config. Should be the same for all nodes in a cluster.
* `conf/cassandra-env.sh`. Sets JVM options. Max recommended heap size: 8G (due to GC pauses and to leave a lot of RAM for OS buffers)
* `conf/log4j-server.properties`

### Multi-Datacentre

Useful for workload separation -- we might want different number of replicas depending on the type of workload, e.g. Solr with 1 and main database with 3.

*Snitch* determines which DC/rack a node belongs to. Configured at node level, has to be the same for all nodes in a cluster. *PropertyFileSnitch* is commonly used; it reads network topology from a file. An alternative is automatic inference from network addresses.

When using multi-datacentre setup the following need to be taken into consideration when defining the schema and upserts/queries: 

* *keyspace*: must set proper replication strategy and set rf by datacentre class: `with replication = { 'NetworkTopologyStrategy', 'DC1': 3, 'DC2': 3 }`
* *consistency level*: we might want to use DC-aware, e.g. local quorum or each quorum.

Cassandra will try to minimise cross-datacentre communication -- coordinator will select one replica in remote datacentre as the local coordinator for that datacentre.

## Tools

### `nodetool`

Useful commands:

* status
* info
* ring

### `cqlsh`

Command-line interface for executing CQL. Replaces `cassandra-cli`.

Useful commands:

* `describe keyspaces;`
* `describe keyspace <keyspace name>`
* `source <script path>`
* `copy <table name> (<column names ...>) from '<csv file path>'`
* `tracing on` -- enable debug tracing of commands

### `cassandra-cli`

Legacy tool, useful for peeking at the physical layout of data.

Useful commands

* `list <table name> limit <limit>`
* `get <table name>[<partition key>]`

### `ccm`

Useful for spinning up a local cluster for development:

TODO: commands to initialise cluster.

~~~
ccm populate -n 3
ccm start
ccm status
~~~

Then `ccm node1 cqlsh` or `ccm node1 cli` will give us `cqlsh`/`cassandra-cli` on given node.

### `cassandra-stress`

For running stress tests. Sample commands:

* `cassandra-stress --threads=8 --num-keys=1000000 --keep-going`
* `cassandra-stress --threads=8 --num-keys=1000000 --keep-going --operation=read`

## Data Model

### Data types

* numbers
* text
* time
* collections
* other (e.g. uuid)

### Storage

Physical row is a set of cells:

partition key  clustered 1  clustered 1     clustered 2   clustered 2
-------------  -----------  --------------  ------------  -----------------
ca58ee3c...    EC:created   EC:start_price  FISI:created  FISI:start_price
               2013-08-29   44.88           2013-08-29    19.69
ca58ee34...    ATR:created  ATR:start_price 
               2013-08-12   4.31

Each cell contains a name, value and timestamp. Cell name can hold data (i.e. a component of the key). The cell are stored in alphabetical order of name (ascending by default; this is controlled by `with clustering order by` clause of `create table`). This allows range queries on components of primary key that are not the partition key.

`cqlsh` hides that from us, but we can take a peek at it using legacy `cassandra-cli`:

How wide can rows be? It has to be stored on a single machine, so better for it not to run into gigabytes. Rule of thumb: no more than 100 MB.

~~~
list watchlistitem limit 2;
~~~

A common pattern for e.g. arbitrary searches is to create a dedicated table with primary key composed of search criteria columns, e.g. *stock_by_industry_by_sector*.

### Primary Key

Key choice: natural key is ok if available. Surrogate is a common choice. Sequences are not supported due to distributed nature. `uuid` and `timeuuid` are available in Cassandra, with the latter providing k-order. Note: `timeuuid` is preferable to `timestamp` when we need the ability to order by time and guarantee of uniqueness but don't need the actual timestamp value.

Compound primary keys are allowed. In such cases the first column of primary key is the *partition key*, the remaining columns are *clustering columns*.

~~~ {.sql}
primary key (stock_symbol, trade_id)
~~~

### Partition Key

A.k.a *row key*. Each partition key designates a single storage engine row, so all writes for a single partition key happen atomically in isolation. In the logical view the row is then split by *primary key* int separate logical rows (groups of columns).

To avoid huge rows we might want to use composite partition keys:

~~~ {.sql}
primary key ((stock_symbol, trade_date), trade_id)
~~~

### Indices

Partition key index (primary index) is used for random access to table data. 

#### Secondary Indices

"Beginner's index". Implemented as an internal, non-replicated table (per node). Not very performant, querying it requires querying all the nodes, becaus each stores a portion of the index that corresponds to the keys it is responsible for. For those reasons they are not recommended in general, but can be useful for low cardinality of indexed values. In other cases we need to maintain our own tables that represent the indices we want to query by.

### Counters

Special column type in CQL. Not completely accurate due to distributed nature and inherent lack of idempotence. Offer good performance at consistency level *one*, at higher levels there is a performance hit due to required reads from multiple replicas. Do not support secondary indices and TTL.

Update:

~~~ {.sql}
update stockcount set watch_count = watch_count + 1 where ...
~~~

### Collections

If we have multiple e-mails per user, in relational db. we'd store them in a separate table. It's not suitable in Cassandra data model. In general we shouldn't be storing more than a couple of thousand elements in a collection due to storage overhead and read/write (in)efficiency.

Each element in a collection can have its own TTL. Collections cannot be nested and cannot be parts of secondary indices.

Three collection types:

#### Set

~~~ {.sql}
create table user ( emails set<text> )
insert into ... values ({'a@b.com', 'b@c.com'})
update user set emails = emails + {'a@b.com'}
update user set emails = emails - {'a@b.com'}
~~~

Storage: all elements of a set are encoded in the key

* `emails:a@b.com`
* `emails:c@d.com`

#### List

~~~ {.sql}
create table user ( emails list<text> )
insert into ... values (['a@b.com', 'b@c.com'])
update user set emails = emails + {'a@b.com'} -- append
update user set emails = {'a@b.com'} + emails -- prepend
delete emails[0] from user
~~~

#### Map

~~~ {.sql}
create table user ( phone_nos map<varchar, varchar> )
update user set phone_nos['work'] = '12345'
delete phone_nos['work'] from user
~~~

Storage: key in name, value in value

* name: `phone_nos:work` value: `12345`

### Writes

All inserts are actually upserts. There is no way to find out if any data was overwritten as a result of an insert.

#### Batching

* atomic, i.e. the cluster will be retrying the batch until writes for all rows succeed; but:
* not isolated -- others can read the parts already written even if the batch has not been written fully

### Reads

* Materialised views need to be maintained for queries
* Predicates on primary key can only be equality

## Storage and I/O

Read and write paths are separate and should not contend with each other in normal conditions. Under prolonged load they might start competing for I/O.

### Write Path

#### Components

* memtable: a buffer, one per table
* commit log: for durability, append-only, fsyncs from time to time. Only once fsync happens the data is durable. Stored in segments (which get recycled) in chronological order.
* sstable: (sorted string table) files on the disk, write-once.

#### Commit log fsync policies

* Periodic (`CommitLogSyncPeriodicInMS`) -- default; the default time is 10 s.
* Batch (`CommitLogSyncBatchWindowInMS`) -- only acks after fsync happened; suitable when we are writing in batches.

#### Operation 

In each node taking part in a write:

1. write to memtable
2. write to commit log (in memory, no fsync required)
3. signal success to coordinator

When limit is hit for a memtable, the data is transferred to a flush queue which gets flushed to disk in one sequential, efficient operation. The sstable file gets written and is never changed after that; new data (and updates) will be written to a new file. The writes are reconciled in memtable: if two competing updates are received before a flush only one of them will get written to sstable.

On startup commit log gets replayed and memtables are recreated.

#### Failure/slowness reasons:

* commit log is full
* OOM
* data partition is full
* lost a replica set (`UnavailableException`)
* consistency is too high

#### Compaction 

Cassandra is designed to absorb load spikes very well. It will just write a lot of separate sstables. In order to maintain good read performance and get rid of removed data it periodically needs to combine the data from these files. Compaction runs periodically and reads sstables from disk, finds matching partition keys, combines the data and writes into a single file. Can be triggered by flush.

A compaction strategy selects which files to compact, collates the partition keys, merges rows and writes new sstables. Strategies:

* `SizeTieredCompactionStrategy` (default) -- triggered by appearance of more than 4 files of similar size they are compacted. Simple, but we have no control over when it will happen. Can be triggered with. **Requires 50% of disk space free** as during compaction there might be temporary duplication of all the data.
* `LeveledCompactionStrategy` -- improves predictability of read performance. Level 0 is initial flush; each subsequent level is 10 times larger than the previous one and from level 1 on there is a fixed sstable size (120MB by default) -- unless a single partition is bigger, in which case it will be in its own file larger than the limit. Invariant for each level: each partition key will be in max one file -- compactions maintain this invariant. Since Cassandra 2.0 it also does size tiered compaction on level 0. Good for read-heavy workload with updates/deletes, but there is I/O overhead so it might struggle keeping up with write-heavy workload.

Choice of strategy is complex and requires careful analysis.

`nodetool compact` runs a full compaction and creates a single large file per table. Not recommended (unless scheduled regularly) since it prevents regular compaction that uses size tiered strategy from happening.

In addition to these, there is also tombstone compaction that runs when the number of TTL=configured and deleted columns exceeds configured threshold. It runs on a single file and only if all partitions in the file are entire contained in that file.

Tuning the performance of compaction might require setting JVM options such as thread priorities in `cassandra-env.sh`.
 
### Read Path

#### Participating Components

In the order of access:

* row cache (off heap) -- cache of slices read, entire rows at once; usually disabled or small, because BufferCache or mmap is more efficient. Only suitable for small rows accessed frequently. To enable: `create table user (...) with caching = 'all'`
* memtable (on heap)
* bloom filter (off heap) -- specifies which files can contain parts of the row
* key cache (if enabled; on heap) -- position of a given key in data file: `(sstable, (key, location))`. Allows us to avoid seek of the index file. Should always be on, has good hit ratio.
* sstable index summary (on heap) -- position in index file
* primary indices (on disk)
* sstables (on disk)

Note: commit log is not read by regular reads.

#### Performance

Ideally few sstables should be accessed for read. If this is not the case it might mean that compaction is not keeping up and that our write rate is too high for system capacity.

### Data Storage

Directory structure: `data`/*keyspace*/*table*/*files...*

File name schema:

*keyspace*-*table*-*version*-*generation*-*component*

* version: denotes cassandra version; newer versions in general can read older; there is a tool for upgrading sstables.
* generation: higher number = older file
* component:
    * `Data` -- sstable
    * `Index`
    * `Filter` -- bloom filter 

### Snapshots

Hard links to table files; prevent compaction from removing compacted sstables. Snapshots can be created with `nodetool` on per-node basis. They are not queryable, only useful as restoration points.

## Concept Map

~~~ {.dot}
digraph cc {
  rankdir = LR;

  bootstrap -> rebalance;
  capacity -> compaction;
  capacity -> "compound primary key";
  cluster -> "node";
  cluster -> "rebalance";
  compaction -> sstable;
  compaction -> "layered";
  compaction -> "size tiered";  
  consistency -> quorum;
  coordinator -> "hinted handoff";
  "data model" -> "primary key";
  "data model" -> keyspace;
  "data model" -> row;
  decommission -> rebalance;
  distribution -> consistency;
  distribution -> cluster;
  distribution -> resiliency;
  "node" -> bootstrap;
  "node" -> coordinator;
  "node" -> decommission;
  "node" -> gossip;
  "node" -> repair;
  "node" -> seed;
  operation -> "read path";
  operation -> repair;
  operation -> snapshot;
  operation -> "write path";
  "partition key" -> row;
  performance -> "read perf";
  performance -> "write perf";
  "primary key" -> "cell";
  "primary key" -> "compound primary key";
  "primary key" -> "partition key";
  "read path" -> "bloom filter";
  "read path" -> "row cache";
  "read path" -> "memtable";
  "read path" -> "key cache";
  "read path" -> index;
  "read path" -> sstable;
  "read perf" -> compaction;
  repair -> "merkel tree";
  resiliency -> redundancy;
  resiliency -> "commit log";
  resiliency -> "hinted handoff";
  resiliency -> "read repair";
  row -> cell;
  scalability -> sharding;
  scalability -> distribution;
  sharding -> "partition key";
  snapshot -> sstable;
  "write path" -> "commit log";
  "write path" -> memtable;
  "write path" -> sstable;
  "write path" -> index;
  "write perf" -> compaction;
}
~~~

## Admin

* compaction: `nodetool -h <address> -p <port> compact` -- runs a full compaction, as opposed to incremental one that runs automatically (see [compaction](#compaction))
* cleanup: `nodetool -h <address> -p <port> cleanup` -- removes redundant data from node, e.g. after a new node joined cluster and took over a replica
* decommissioning a node: `nodetool -h <address> -p <port> decommission` 
* repair: `nodetool -h <address> -p <port> repair` -- required to sort out deletes, so should be scheduled for entire cluster. Expensive, should be rolling through the nodes, not on all at once. Also required after changing replication factor. Takes long time to run.
* remove node: `nodetool removenode <node id>` -- when node goes down, reallocates key ranges but does not cause the cluster to move the data around accordingly. Repair is needed for that.

## Benchmark

Writing 100m small rows (5 cols, char(1)/bigint/decimal/timestamp) rows to an unindexed table on an SSD:

* 9 GB (8 GB not counting commit logs and saved caches) on disk
* 4h 51m

`select count(*) from postings limit 100000000;` after the writes described above took ~10 mins to return.

`create index account_idx on postings (account_id)` is asynchronous -- kicks off indexing in the background. Took over 8 hours, the index added 3 GB to disk usage.

## Usage

* [Instagram](http://planetcassandra.org/blog/post/instagram-making-the-switch-to-cassandra-from-redis-75-instasavings): auditing data, 12-node cluster of extra large EC2 instances, storing 1.2 TB across the cluster, peak writes 20k/s, reads 15k/s.
* Netflix: 500 nodes, 30 clusters, 65 TB

## References

* DataStax 2.0 [docs](http://www.datastax.com/documentation/cassandra/2.0/webhelp/)
* [Blog post](http://bigdatanoob.blogspot.co.uk/2012/11/hbase-vs-cassandra.html) comparing Cassandra and HBase
* [Blog post](http://planetcassandra.org/blog/post/composite-keys-in-apache---cassandra) on composite primary keys
* [Big Data Partnership](http://www.bigdatapartnership.com) course
* Call me Maybe [blog post](http://aphyr.com/posts/294-call-me-maybe-cassandra/)
