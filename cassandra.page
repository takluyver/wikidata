---
categories: it database nosql
...

All writes are timestamped at the client and later overwrite (upsert) on top of the former, so [requires clocks to be synchronised on all clients](http://www.datastax.com/documentation/cassandra/2.0/webhelp/index.html#cassandra/dml/dml_about_inserts_c.html#concept_ds_xvr_knw_zj).

Does not support range scans.

## Architecture 

### Overview

* many nodes
* client requests always based on a key

~~~ {.asciiart}
+------+
|Client|
|------|
|Driver|
+------+
  ^
  |
  v
+---------node 1-+     +--node 2-+
|API             |     |         |
|----------------|     |         |
|cluster-aware  <------->        |
|----------------|     |         |
|cluster-unaware |     |         |
|----------------|     |         |
|disk            |     |         |
+----------------+     +---------+
~~~

* All nodes are equal, no masters.
* Peer-to-peer over gossip protocol: no full mesh, every node picks three random nodes every second and communicates with them.
* Transport mechanism: historically: [Thrift](Apache Thrift). Since 1.2 a native binary protocol which is much more flexible and efficient. Native is recommended.
* Log storage -- no in-place updates, append only, allows for very fast writes
* manages its own storage locally -- tight integration with storage (unlike HBase), allows optimisations
* new nodes can be added while online

### Cluster Structure

A node joins a cluster based on its configuration. There is a seed list of nodes from which it will bootstrap its communication. After a while it will be aware of the entire cluster. 

The nodes are organised in a logical ring that represents the hash space in [DHT](distributed hash table). Each node receives a token that corresponds to a specific hash range. The hash function is configurable:

* Murmur3Partitioner -- fastest, recommended
* RandomPartitioner
* ByteOrderPartitioner -- deprecated, not recommended

When client sends a request it is sent to a node (which one depends on policy, random by default) and that node assumes the role of a coordinator. If that node fails another node will be contacted. The coordinator computes the partition key using hash function and communicates with the node which holds the token for that partition key (and all replicas). It will participate in all the communication between the client and nodes responsible for the data. New drivers can be aware of cluster topology and can send requests directly to node responsible for the partition.

### Replication

Replication is controlled by two parameters: *replication factor* and *replication strategy*. 

Replication factor simply determines how many replicas there are. 3 is a reasonable default. It is set per keyspace and in general should not be changed in existing cluster.

Placement of replicas is determined by replication strategy:

* SimpleStrategy -- first replicat determined by partitioner, additional are next n nodes clockwise in the ring; only suitable for single datacentre, not recommended for production
* NetworkTopologyStrategy -- rack/datacentre aware placement of replicas; specifies how many replicas in each datacentre, useful if there are e.g. different performance requirements in different datacentres.

### Consistency

Updates might not propagate to all replicas immediately. What if a read occurs before an update is propagated to all replicas? There is consistency requirement setting on writes and reads (*tunable consistency*) which determines how many replicas need to respond before we are happy with the result. This can be set at anything from *any* (we don't care how many) to *all* (all replicas have to respond, i.e. immediate consistency). *Quorum* is typically a reasonable default that offers good writer performance and read consistency, if used for both writes and reads. In general, if *read consistency level* + *write consistency level* > *replication factor* we get strong consistency.

What happens when a replica is down? Coordinator stores a hint that a given write needs to be sent and sends them when replica comes back online (*hinted handoff*). What if the coordinator dies and the hints are lost? Read-repair addresses it.

## Configuration

* `conf/cassandra.yaml`. Main config. Should be the same for all nodes in a cluster.
* `conf/cassandra-env.sh`. Sets JVM options. Max recommended heap size: 8G (due to GC pauses and to leave a lot of RAM for OS buffers)
* `conf/log4j-server.properties`

## Tools

### `nodetool`

Useful commands:

* status
* info
* ring

### `cqlsh`

Command-line interface for executing CQL. Replaces `cassandra-cli`.

## Data Layout

### Composite Primary Keys

According to [a blog post](http://planetcassandra.org/blog/post/composite-keys-in-apache---cassandra) with single-column primary key the data is stored one record per row. With composite primary key it's stored one record per column, i.e. all records in one row. It's not clear what implications it has and whether this is still the case with Cassandra 2 (the post was about version 1).

## Benchmark

Writing 100m small rows (5 cols, char(1)/bigint/decimal/timestamp) rows to an unindexed table on an SSD:

* 9 GB (8 GB not counting commit logs and saved caches) on disk
* 4h 51m

`select count(*) from postings limit 100000000;` after the writes described above took ~10 mins to return.

`create index account_idx on postings (account_id)` is asynchronous -- kicks off indexing in the background. Took over 8 hours, the index added 3 GB to disk usage.

## Usage

* [Instagram](http://planetcassandra.org/blog/post/instagram-making-the-switch-to-cassandra-from-redis-75-instasavings): auditing data, 12-node cluster of extra large EC2 instances, storing 1.2 TB across the cluster, peak writes 20k/s, reads 15k/s.

## References

* DataStax 2.0 [docs](http://www.datastax.com/documentation/cassandra/2.0/webhelp/)
* [Blog post](http://bigdatanoob.blogspot.co.uk/2012/11/hbase-vs-cassandra.html) comparing Cassandra and HBase
* [Blog post](http://planetcassandra.org/blog/post/composite-keys-in-apache---cassandra) on composite primary keys
* Big Data Partnership course
