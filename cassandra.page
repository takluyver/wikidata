---
categories: it database nosql
...

## Documentation

* cassandra.apache.org outdated
* DataStax most up to date

## Architecture 

### Overview

* many nodes
* client requests always based on a key

~~~ {.asciiart}
+------+
|Client|
|------|
|Driver|
+------+
  ^
  |
  v
+---------node 1-+     +--node 2-+
|API             |     |         |
|----------------|     |         |
|cluster-aware  <------->        |
|----------------|     |         |
|cluster-unaware |     |         |
|----------------|     |         |
|disk            |     |         |
+----------------+     +---------+
~~~

* All nodes are equal, no masters.
* Peer-to-peer over gossip protocol: no full mesh, every node picks three random nodes every second and communicates with them.
* Transport mechanism: historically: [Thrift](Apache Thrift). Since 1.2 a native binary protocol which is much more flexible and efficient. Native is recommended.
* Log storage -- no in-place updates, append only, allows for very fast writes
* manages its own storage locally -- tight integration with storage (unlike HBase), allows optimisations
* new nodes can be added while online, but it should be done before the cluster is close to running out of capacity

### Cluster Structure

A node joins a cluster based on its configuration. There is a seed list of nodes from which it will bootstrap its communication. After a while it will be aware of the entire cluster. 

The nodes are organised in a logical ring that represents the hash space in [DHT](distributed hash table). Each node receives a token that corresponds to a specific hash range. The hash function is configurable:

* Murmur3Partitioner -- fastest, recommended
* RandomPartitioner
* ByteOrderPartitioner -- deprecated, not recommended

When client sends a request it is sent to a node (which one depends on policy, random by default) and that node assumes the role of a coordinator. If that node fails another node will be contacted. The coordinator computes the partition key using hash function and communicates with the node which holds the token for that partition key (and all replicas). It will participate in all the communication between the client and nodes responsible for the data. New drivers can be aware of cluster topology and can send requests directly to node responsible for the partition.

Virtual Nodes: instead of being responsible for a single token, node can get a number (256 is a good default) of tokens. It makes admin tasks such as rebalancing of the cluster much easier.

### Replication

Replication is controlled by two parameters: *replication factor* and *replication strategy*. 

Replication factor simply determines how many replicas there are. 3 is a reasonable default. It is set per keyspace and in general should not be changed in existing cluster.

Placement of replicas is determined by replication strategy:

* SimpleStrategy -- first replicat determined by partitioner, additional are next n nodes clockwise in the ring; only suitable for single datacentre, not recommended for production
* NetworkTopologyStrategy -- rack/datacentre aware placement of replicas; specifies how many replicas in each datacentre, useful if there are e.g. different performance requirements in different datacentres.

### Consistency

Updates might not propagate to all replicas immediately. What if a read occurs before an update is propagated to all replicas? There is consistency requirement setting on writes and reads (*tunable consistency*) which determines how many replicas need to respond before we are happy with the result. This can be set at anything from *any* (we don't care how many) to *all* (all replicas have to respond, i.e. immediate consistency). *Quorum* offers strong consistency, if used for both writes and reads. In general, if *read consistency level* + *write consistency level* > *replication factor* we get strong consistency.

Levels useful in practice:

* *any* -- only applies to writes; for data that we can lose
* *one* -- high availability and performance
* *quorum* -- strong consistency
* *local quorum* -- minimises latency with within-datacentre consistency
* *each quorum* -- ensures consistency in the face of total datacentre loss

Noone uses *all* in practice; if we need that level of consistency we are typically better off with a different product.

What happens when a replica is down? Coordinator stores a hint that a given write needs to be sent and sends them when replica comes back online (*hinted handoff*). What if the coordinator dies and the hints are lost? Read-repair addresses it. Note that hinted handoff only applies to situation when consistency level of write is lower than *all*. We always have the option to retry in the app if write at high consistency level fails and we need assurance about consistency.

### Gossip

Each node has a list of seed nodes it will try to contact in order to bootstrap. Every second every node picks three random nodes and sends the latest information it has about all nodes in the cluster (e.g. their key ranges, ping time etc.). Protocol described in more detail in Dynamo paper.

### Read Repair

When read is requested with consistency lower than all, the responses from fastest replicas will be returned to the client. The coordinator will also send a repair request to other replicas; they will compare their data with a checksum provided by the fastest replicas and if there is a mismatch will attempt to repair.

## Configuration

* `conf/cassandra.yaml`. Main config. Should be the same for all nodes in a cluster.
* `conf/cassandra-env.sh`. Sets JVM options. Max recommended heap size: 8G (due to GC pauses and to leave a lot of RAM for OS buffers)
* `conf/log4j-server.properties`

### Multi-Datacentre

Useful for workload separation -- we might want different number of replicas depending on the type of workload, e.g. Solr with 1 and main database with 3.

*Snitch* determines which DC/rack a node belongs to. Configured at node level, has to be the same for all nodes in a cluster. *PropertyFileSnitch* is commonly used; it reads network topology from a file. An alternative is automatic inference from network addresses.

When using multi-datacentre setup the following need to be taken into consideration when defining the schema and upserts/queries: 

* *keyspace*: must set proper replication strategy and set rf by datacentre class: `with replication = { 'NetworkTopologyStrategy', 'DC1': 3, 'DC2': 3 }`
* *consistency level*: we might want to use DC-aware, e.g. local quorum or each quorum.

Cassandra will try to minimise cross-datacentre communication -- coordinator will select one replica in remote datacentre as the local coordinator for that datacentre.

## Tools

### `nodetool`

Useful commands:

* status
* info
* ring

### `cqlsh`

Command-line interface for executing CQL. Replaces `cassandra-cli`.

Useful commands:

* `describe keyspaces;`
* `describe keyspace <keyspace name>`
* `source <script path>`
* `copy <table name> (<column names ...>) from '<csv file path>'`
* `trace on` -- enable debug tracing of commands

### `cassandra-cli`

Legacy tool, useful for peeking at the physical layout of data.

Useful commands

* `list <table name> limit <limit>`
* `get <table name>[<partition key>]`

### `ccm`

Useful for spinning up a local cluster for development:

TODO: commands to initialise cluster.

~~~
ccm populate -n 3
ccm start
ccm status
~~~

Then `ccm node1 cqlsh` or `ccm node1 cli` will give us `cqlsh`/`cassandra-cli` on given node.

## Data Model

### Data types

* numbers
* text
* time
* collections
* other (e.g. uuid)

### Storage

Physical row is a set of cells:

partition key  clustered 1  clustered 1     clustered 2   clustered 2
-------------  -----------  --------------  ------------  -----------------
ca58ee3c...    EC:created   EC:start_price  FISI:created  FISI:start_price
               2013-08-29   44.88           2013-08-29    19.69
ca58ee34...    ATR:created  ATR:start_price 
               2013-08-12   4.31

Each cell contains a name, value and timestamp. Cell name can hold data (i.e. a component of the key). The cell are stored in alphabetical order of name (ascending by default; this is controlled by `with clustering order by` clause of `create table`). This allows range queries on components of primary key that are not the partition key.

`cqlsh` hides that from us, but we can take a peek at it using legacy `cassandra-cli`:

How wide can rows be? It has to be stored on a single machine, so better for it not to run into gigabytes. Rule of thumb: no more than 100 MB.

~~~
list watchlistitem limit 2;
~~~

A common pattern for e.g. arbitrary searches is to create a dedicated table with primary key composed of search criteria columns, e.g. *stock_by_industry_by_sector*.

### Primary Key

Key choice: natural key is ok if available. Surrogate is a common choice. Sequences are not supported due to distributed nature. `uuid` and `timeuuid` are available in Cassandra, with the latter providing k-order. Note: `timeuuid` is preferable to `timestamp` when we need the ability to order by time and guarantee of uniqueness but don't need the actual timestamp value.

Compound primary keys are allowed. In such cases the first column of primary key is the *partition key*, the remaining columns are *clustering columns*.

~~~ {.sql}
primary key (stock_symbol, trade_id)
~~~

### Partition Key

A.k.a *row key*. Each partition key designates a single storage engine row, so all writes for a single partition key happen atomically in isolation. In the logical view the row is then split by *primary key* int separate logical rows (groups of columns).

To avoid huge rows we might want to use composite partition keys:

~~~ {.sql}
primary key ((stock_symbol, trade_date), trade_id)
~~~

### Indices

Partition key index (primary index) is used for random access to table data. 

#### Secondary Indices

"Beginner's index". Implemented as an internal, non-replicated table (per node). Not very performant, querying it requires querying all the nodes, becaus each stores a portion of the index that corresponds to the keys it is responsible for. For those reasons they are not recommended in general, but can be useful for low cardinality of indexed values. In other cases we need to maintain our own tables that represent the indices we want to query by.

### Counters

Special column type in CQL. Not completely accurate due to distributed nature and inherent lack of idempotence. Offer good performance at consistency level *one*, at higher levels there is a performance hit due to required reads from multiple replicas. Do not support secondary indices and TTL.

Update:

~~~ {.sql}
update stockcount set watch_count = watch_count + 1 where ...
~~~

### Collections

If we have multiple e-mails per user, in relational db. we'd store them in a separate table. It's not suitable in Cassandra data model. In general we shouldn't be storing more than a couple of thousand elements in a collection due to storage overhead and read/write (in)efficiency.

Each element in a collection can have its own TTL. Collections cannot be nested and cannot be parts of secondary indices.

Three collection types:

#### Set

~~~ {.sql}
create table user ( emails set<text> )
insert into ... values ({'a@b.com', 'b@c.com'})
update user set emails = emails + {'a@b.com'}
update user set emails = emails - {'a@b.com'}
~~~

Storage: all elements of a set are encoded in the key

* `emails:a@b.com`
* `emails:c@d.com`

#### List

~~~ {.sql}
create table user ( emails list<text> )
insert into ... values (['a@b.com', 'b@c.com'])
update user set emails = emails + {'a@b.com'} -- append
update user set emails = {'a@b.com'} + emails -- prepend
delete emails[0] from user
~~~

#### Map

~~~ {.sql}
create table user ( phone_nos map<varchar, varchar> )
update user set phone_nos['work'] = '12345'
delete phone_nos['work'] from user
~~~

Storage: key in name, value in value

* name: `phone_nos:work` value: `12345`

### Writes

All inserts are actually upserts. There is no way to find out if any data was overwritten as a result of an insert.

#### Batching

* atomic, i.e. the cluster will be retrying the batch until writes for all rows succeed; but:
* not isolated -- others can read the parts already written even if the batch has not been written fully

### Reads

* Materialised views need to be maintained for queries
* Predicates on primary key can only be equality

## Benchmark

Writing 100m small rows (5 cols, char(1)/bigint/decimal/timestamp) rows to an unindexed table on an SSD:

* 9 GB (8 GB not counting commit logs and saved caches) on disk
* 4h 51m

`select count(*) from postings limit 100000000;` after the writes described above took ~10 mins to return.

`create index account_idx on postings (account_id)` is asynchronous -- kicks off indexing in the background. Took over 8 hours, the index added 3 GB to disk usage.

## Usage

* [Instagram](http://planetcassandra.org/blog/post/instagram-making-the-switch-to-cassandra-from-redis-75-instasavings): auditing data, 12-node cluster of extra large EC2 instances, storing 1.2 TB across the cluster, peak writes 20k/s, reads 15k/s.
* Netflix: 500 nodes, 30 clusters, 65 TB

## References

* DataStax 2.0 [docs](http://www.datastax.com/documentation/cassandra/2.0/webhelp/)
* [Blog post](http://bigdatanoob.blogspot.co.uk/2012/11/hbase-vs-cassandra.html) comparing Cassandra and HBase
* [Blog post](http://planetcassandra.org/blog/post/composite-keys-in-apache---cassandra) on composite primary keys
* Big Data Partnership course
* Call me Maybe [blog post](http://aphyr.com/posts/294-call-me-maybe-cassandra/)
