

## A Practical Theory of Language-Integrated Query

*Philip Wadler*

Most important thing about functional languages: they are DSLs (for creating other DSLs). That is because lambda abstraction is built in and the way they work makes it easy to decompose problems. One of the first papers on FP: Landin's "the next 700 programming languages". Worth reading, as well as McCarthy's original Lisp paper.

* Links (the Links in Edinburgh; at the other end of the meadows from Hope; Hope is an early FP language) -- nice research project
* at the same time Erik Meijer et al. released LINQ -- working thing

How do you compare the two? Build a theory!

Table comparing -- F# 2, 3 and their theory: the theory always works and can be much faster, there is a theorem.

Rejected from DB conference as too theoretical.

Basic idea: **quotation**.

~~~ {.fsharp}
{people = [
	{name = "Alex", age = 60};
	{name = "Bert", age = 56};
	{name = "Cora", age = 33};
	{name = "Drew", age = 31};
	{name = "Edna", age = 21};
	{name = "Fred", age = 60};]}

type DB = {people: {name: string; age: int} list}

type Names = {name: string} list

let db: Expr<DB> = <@ database("People") @>
~~~

Problems with this representation:

* some databases don't fit in memory
* requires full scans

`<@ ... @>` is a quotation -- the parse tree of the expression between the brackets

`<@ database("People") @>` denotes expression that, if executed, reads in the entire database.

~~~ {.fsharp}
let range(a: int, b: int): Expr<Names> =
  <@ for w in (%db).people do
     if (%lift(a)) <= w.age && w.age < (%lift(b)) then
     yield {name: w.name} @>
~~~

now, `run(range(30, 40))` will produce something isomorphic to

~~~ {.sql}
select w.name as name
from people as w
where 30 <= w.age and w.age < 40
~~~

* `%` -- splice: inserts a parse tree at this place
* `lift` -- makes

More difficult in F#:

~~~ {.fsharp}
let satisfies: Expr<(int -> bool) -> Names> =
  <@ fun(p) -> for w in (%db).people do
     if p(w.age) then
     yield {name: w.name} @>
~~~

`run(<@(%satisfies)(fun(x) -> 30 <=x && x < 40))` produces the same query as before.

How about efficiency? We don't care -- the cost is in executing the query, not generating it. We should care about being able to clearly express the intent.

Aside: scala uses type-based quotation which are more fragile than direct quotations.

Now for something really cool: dynamic query generation.

~~~ {.fsharp}
type Predicate = 
  | Above of int
  | Below of int
  | And of Predicate * Predicate
  | Or of Predicate * Predicate
  | Not Predicate

let t_0: PRedicate = And(Above(30), Below(40))

let rec P(t: Predicate): Expr<int -> bool> =
  match t with
    | Above(a) -> <@ fun(x) -> (%lift(a)) <= x @>
    // ...
    | Not(t)   -> <@ fun(x) -> not(%P(t)(x)) @>
~~~

This is very, very hard in LINQ.

What our technique supports:

* Join queries
* Abstractioin over values
* Abstraction over predicates
* Composition of queries
* Dynamic gneeration of queries
* Nested intermediate data
* Compiling XPath to SQL

> Don't be afraid of maths. It's a candle in the darkness!

* closed quotation: `Expr<A -> B>` 
* open quotation: `Expr<A> -> Expr<B>`

Note: F# only has closed quotations:.

### Theory

* ordinary typing rules for lambda calculus plus `for`, `run` and `lift`, `database`, `antiquote`
* normalisation rules -- symbolic evaluation; well known rules, always terminate
* normalisation for ad-hoc rewriting (to generate legal SQL) -- need 
* theorem: if L has type A then ...

### Practice

Normalising the query never makes it worse. Published P-LINQ library.

### Q&A

* Erweb makes even stronger guarantees, but requires much more powerful type system

### Lesson

Quotations are amenable to static analysis and theoretical treatment.



## ParaForming: Forming Parallel Functional Programs Using Refactoring

*Kevin Hammond*

1985: 12-40 MHz
1993 Pentium: 60-300 MHz
2000: Pentium 4 1.3-3.6 GHz

-- frequency wall: power usage increases with cube of frequency. 4 GHz CPU never went into prod, 300 Watts

2006: Core Duo 1.8-3.33 GHz
2013: Xeon Phi: 60 cores at 1 GHz

### Near Future

* megacore, upt to millions of small cores
* with hundreds of dedicated lightweight integer units and separate floating point units. Likely to be heterogeneous.
* not even NUMA, no shared memory at all -- Xeon 5 has no shared memory; message passing between cores, each core running a mini-linux

Samsung Eynos 5 Octa: 8 cores, 4 of which are dark 4 are high-performance, 4 low performance

> If we don't solve the multicore challenge, then no other advances will matter!

3^8 threads created in 5 seconds on 16 core machine. 20k of them were actually executed.

Wrote the first parallel haskell compiler.

> Not every problem can't be prallelised

Really? (example with building a brick wall)

* Linear speedup is extremely hard to achieve (Amdahl's law)
* The trick to recognise dependencies and break them down to strong ones

How do we make programmers think parallel? New high level programming constructs for dealing with millions of threads.

* deadlocks must be eliminated by design
* preformance informatin must be included as part of the design -- we can't afford "correct first, optimal later"
* existing algorithms often don't parallelise well -- need to invent new ones from scratch

Bob Harper: The only thing that works for parallel is FP

concurrency: illusion; parallelism: the real thing

ParaForm:

identify components using semi-automatic refactoring
identify common patterns (TODO: photo)
data parallelism/task parallelism

Skel library for Erlang: fully nestable DSL for parallelism. Primitives such as 
* `pipe`
* `farm`

ParaPhrase: takes a program and transforms it into code that can be run on heterogeneous parallel machine.

Could be built into IDE: inspections that suggest transformation and refactorings to transform the code.

Example of Wrangler (Emacs-based Erlang refactoring tool).

* productivity improved 10-fold compared to manual refactoring while giving the same performance.

Dependent types allow us to use simple definition of aparllel skeletons at type level.

Implemented in Eclipse for C++(11)

Book on Skeleton-based approach in Haskell is in the works.

[http://www.paraphrase-ict.eu]()

[http://www.project-advance.eu]()


## Build Your Own Lisp for Great Justice

*Bodil Stoke*

Bodol: Clojure + Haskell; pure, immutable, homoiconic, typed

~~~ {.lisp}
(f fac
  0 -> 1
  n -> (* n (fac (- n 1))))
~~~

Built in clojure. Uses instaparse for parsing, which takes in EBNF.

Elementary functions (after MacCarthy): `cons`, `car`, `cdr`, `cond`, `eq`, `atom`, `lambda`, `lavel`, `quote` 

Eval based on state monad (demo of how to do it in clojure: `reduce-state`)

Pattern matching built using `core.logic`.

Type system also built `core.logic`. Not good, because doesn't give reasons for type errors, just says "no".

~~~ {.clojure}
(run ! [succ]
  (fresh [a b +]
         (== + [:fun :number :number :number]))
         (== succ [:fun a b])
         (== + [:fun a :number b]))
~~~

How to name:

* person's name (Haskell)
* CS concept
* append "Script" to a popular language
* leter increment


## Data Science using FP

*Amanda Launcher*

(Neo4j product demo)

data science: study of generalisable extraction of knowledge from data

science: proving stuff

RDBMS + function chaiing is great to determine a single fact, but changing the functions requires reprocessing everything. Also not very friendly to data scientists.

data complexity = f(size, semi-structure, connectedness)

semi-structure: sparse tables
connectedness: join tables

graph databases are good for densely-connected, semi-structured domains

* nodes: entities; (instances instead of classes of data)
* edges: relationships; can have properties
* labels: e.g. for marking what type of node it is (Person, Address etc.)

### Query Language: Cypher

Matching pattern

~~~
()-->()
(:Person)-[:FRIEND]->(:Person)
(:Person{name:'Peter'})-[:FRIEND]->(:Person{name:'Lucy'})
(p1:Person{name:'Peter'})-[r:FRIEND]->(p2:Person{name:'Lucy'})
(p1:Person{name:'Peter'})-[r:FRIEND*1..2]->(p2:Person{name:'Lucy'})
~~~

Example query:

~~~
match (:Person{name:'Peter'})-[:FRIEND]->(friend)
return friends
~~~

~~~
match (n) return n limit 100
~~~

Sample applications:

* dating recommendations
* fraud detection: taking loans without intention to repay; can associate people with identification info and find shared bits of id to identify fraud rings.
* logistics: calculating optimal route for parcel deliveries (demo didn't work)


## Teaching an Old Dog New Tricks: Wrapping Imperative API in a Functional One

*Chris Marshall*

Experience report (as opposed to a review/survey) on putting Scala API on top of Java one.

* Furnace -- a java system NIO-based on top of RDBMS
* transaction id + events to create new one, update existing one etc

What we want:

* Tagged types for transaction id and event id
* purity

What we need:

* trampolines 
* `unsafePerformIO`
* `EitherT`

Q (Wadler): so how should we design a functional API wrappers?

A: in Scala/Java, it's not hard -- there are mechanisms for dealing with external resources


## Railway Oriented Programming

*Scott Wlaschin*

Happy path programming -- what we see in most FP tutorials: we can just compose a bunch of functions:

* receive request
* validate and canonicalise request
* update existing user record
* send verification email
* return result to user

But what when an error occurs somewhere in that chain?

* samples of silly errors, hilarious

In imperative code: `try {...} catch {...}`, `if (success) {...}` -- typically 200% extra lines of code, ugly as well

In functional: `Either Success Failure`.

Another monad tutorial -- with railway analogy:

~~~ {.asciiart}
 validate     update  ...
--*---success--*----  ...
   \            \
----*--failure---*--- ...
~~~

How do we turn

~~~ {.asciiart}
---*------
    \
     -----
~~~

into

~~~ {.asciiart}
---*------
    \
-----*----
~~~

?

`bind` (>>=)

### Categorising errors

~~~ {.haskell}
data ErrorMessage = EmailMustNotBeBlank
                  | EmailNotValid EmailAddress
                  | SmtpTimeout SmtpConnection
                  | DbUserNotFoundError UserId
                  | ... -- everything that can go wrong

data Result t = Success t
              | Failure ErrorMessage
~~~

List of all errors is useful -- we can discuss with users what should happen, we have to deal with all of them anyway.

If we have separate modules we might need to map errors from one to errors in the other.


## Mirage

*Anil Madhavapeddy*

Cambridge Uni.

* Compositionality is difficult in distributed systems.
* APIs such as POSIX are difficult to evolve. The illusion of "standards" no longer holds, we needs bits of POSIX and bits of OS-specific code.
* Lets assume we have a clean slate and see where we can go
* why OCaml? Powerful module system very useful for programming at scale. We can build applications as functors across OS functionality such as I/O
* develop on Unix but compile on specialised 
* Mirage: 1MB microkernel, including device drivers, TCP and HTTP stack; can run on Xen or public cloud

Current virtual appliances: host OS -> guest OS -> app

Cloud: we don't need all of this flexibility: separate device for web server, db etc. The compiler will link in the network stack, device drivers etc. analyse and optimise everything together. Hypervisor gives us a stable hardware interface.

OCaml modules let us abstract over OS functionality

~~~ {.asciiart}
       TCP(ETH)
         |
         v
APP <- HTTP <- TCP <- UNIX
~~~

SAT-solver picks the right modules for a specific target. All libraries are small and focused and compose nicely.

Mirage: library operating system; no isolation between processes, but a lot of control and performance. Flat address space, guraranteed contiguous heap.

The only monad: for concurrency (cooperative threading). Backend on top of Unix system calls or on top of Xen.

Performance

* Incredibly small appliances images: web server under 200 KB.
* Boot time: 10 ms -- you can boot up a VM for every incoming connection!

opam: precise dependency management:

source + config ---opam---> appliance

personal hosting FTW


## QuickChecking Riak

*John Hughes*

Testing API: random sequence of API calls, if it fails, we shrink the list of calls to minmal one -- reduce noise.

Riak: how do you test eventual consistency? If at the end of the test it's not consistent, perhaps it's not "eventually" yet?

Start with one key: our model of the state of the system involves just one value, that is stored under the key. Now we can generate a sequence of `put` and `get` calls  and compare actual results with computed model.

* discovered that put with new object leads to loss of data due to vector clocks

We don't want to put vector clocks in our model; instead, we record what was each client's last view of the value and add a rule that if we have already interacted with a key then we must update it, not create a new object.

Conflict resolution: in case of write conflict conflicting values are stored and all are returned to the client on next get. Need to include that in the model. That gets complicated because of vector clock optimisation (one clock per collection). But our model gets simpler as a result.

Failures can be modelled by arbitrary node fallbacks on every API call and then performing all handofs.

QuickCheck produced minimal scenario (API calls and network splits) that lead to data loss.

G -> QuickCheck poolboy (80 unit tests, all passing, lots of issues found)

Experiment with students: coming up with QC properties is difficult, but if we are able to do this then they are much more effective at finding issues than unit tests.

Shrinking in presence of non-determinism: run shrunk case lots of times; if it fails at least once it's a valid counterexample.


## Park Bench
